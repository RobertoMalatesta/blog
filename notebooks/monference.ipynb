{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import apollocaffe\n",
    "from apollocaffe.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hmm = namedtuple(\"Hmm\", [\"n_hidden\", \"n_observed\", \"transitions\", \"emissions\"])\n",
    "\n",
    "def make_hmm(n_hidden, n_observed):\n",
    "    transitions = np.random.random((n_hidden, n_hidden))\n",
    "    emissions = np.random.random((n_hidden, n_observed))\n",
    "    \n",
    "    for i in range(n_hidden):\n",
    "        transitions[i, :] /= transitions[i, :].sum()\n",
    "        emissions[i, :] /= emissions[i, :].sum()\n",
    "        \n",
    "    return Hmm(n_hidden, n_observed, transitions, emissions)\n",
    "\n",
    "def sample(n_steps, hmm):\n",
    "    current_state = np.random.choice(hmm.n_hidden)\n",
    "    hidden = []\n",
    "    observed = []\n",
    "    for i in range(n_steps):\n",
    "        hidden.append(current_state)\n",
    "        observed.append(np.random.choice(hmm.n_observed, \n",
    "                                         p=hmm.emissions[current_state, :]))\n",
    "        current_state = np.random.choice(hmm.n_hidden,\n",
    "                                         p=hmm.transitions[current_state, :])\n",
    "        \n",
    "    return hidden, observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward(observed, hmm, viterbi=False):\n",
    "    chart = np.ones((hmm.n_hidden, len(observed)))\n",
    "    for i in range(len(observed)):\n",
    "        if i > 0:\n",
    "            if viterbi:\n",
    "                for j in range(hmm.n_hidden):\n",
    "                    chart[j, i] *= (hmm.transitions[:, j] * chart[:, i-1]).max()\n",
    "            else:\n",
    "                chart[:, i] *= hmm.transitions.T.dot(chart[:, i-1])\n",
    "        chart[:, i] *= hmm.emissions[:, observed[i]]\n",
    "    return chart\n",
    "        \n",
    "def backward(observed, hmm, viterbi=False):\n",
    "    chart = np.ones((hmm.n_hidden, len(observed)))\n",
    "    for i in range(len(observed)-1, -1, -1):\n",
    "        if i < len(observed) - 1:\n",
    "            prev = chart[:, i+1] * hmm.emissions[:, observed[i+1]]\n",
    "            if viterbi:\n",
    "                for j in range(hmm.n_hidden):\n",
    "                    chart[j, i] *= (hmm.transitions[j, :] * prev).max()\n",
    "            else:\n",
    "                chart[:, i] *= hmm.transitions.dot(prev)\n",
    "    return chart\n",
    "            \n",
    "def forward_backward(observed, hmm, viterbi=False):\n",
    "    chart_f = forward(observed, hmm, viterbi)\n",
    "    chart_b = backward(observed, hmm, viterbi)\n",
    "    marginals = chart_f * chart_b\n",
    "    return marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_chart_acc(hiddens, observeds, hmm, backward, viterbi):\n",
    "    acc_f, exact_f = 0., 0.\n",
    "    acc_fb, exact_fb = 0., 0.\n",
    "    acc_f_v, exact_f_v = 0., 0.\n",
    "    acc_fb_v, exact_fb_v = 0., 0.\n",
    "    \n",
    "    for i in range(hiddens.shape[0]):\n",
    "        hidden = hiddens[i, :]\n",
    "        observed = observeds[i, :]\n",
    "        chart_f = forward(observed, hmm)\n",
    "        chart_fb = forward_backward(observed, hmm)\n",
    "        chart_f_v = forward(observed, hmm, viterbi=True)\n",
    "        chart_fb_v = forward_backward(observed, hmm, viterbi=True)\n",
    "\n",
    "        pred_f = chart_f.argmax(axis=0)\n",
    "        pred_fb = chart_fb.argmax(axis=0)\n",
    "        pred_f_v = chart_f_v.argmax(axis=0)\n",
    "        pred_fb_v = chart_fb_v.argmax(axis=0)\n",
    "\n",
    "        acc_f += (pred_f == hidden).sum()\n",
    "        acc_fb += (pred_fb == hidden).sum()\n",
    "        acc_f_v += (pred_f_v == hidden).sum()\n",
    "        acc_fb_v += (pred_fb_v == hidden).sum()\n",
    "\n",
    "        exact_f += (pred_f == hidden).all()\n",
    "        exact_fb += (pred_fb == hidden).all()\n",
    "        exact_f_v += (pred_f_v == hidden).all()\n",
    "        exact_fb_v += (pred_fb_v == hidden).all()\n",
    "\n",
    "        \n",
    "        \n",
    "    if viterbi:    \n",
    "        if backward:\n",
    "            return exact_fb_v / hiddens.shape[0]\n",
    "        else:\n",
    "            return exact_f_v / hiddens.shape[0]\n",
    "    else:\n",
    "        n = hiddens.shape[0] * hiddens.shape[1]\n",
    "        if backward:\n",
    "            return acc_fb / n\n",
    "        else:\n",
    "            return acc_f / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Index(PyLayer):\n",
    "    def forward(self, bottom, top):\n",
    "        data, indices = bottom\n",
    "        index_data = indices.data.astype(int)\n",
    "        top[0].reshape(indices.shape)\n",
    "        top[0].data[...] = data.data[range(indices.shape[0]), index_data]\n",
    "\n",
    "    def backward(self, top, bottom):\n",
    "        data, indices = bottom\n",
    "        index_data = indices.data.astype(int)\n",
    "        data.diff[...] = 0\n",
    "        data.diff[range(indices.shape[0]), index_data] = top[0].diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_LAYER = 100\n",
    "\n",
    "net = apollocaffe.ApolloNet()\n",
    "\n",
    "def iter_rnn(hiddens, observeds, hmm):\n",
    "    net.clear_forward()\n",
    "    \n",
    "    seq_len = hiddens.shape[1]\n",
    "    \n",
    "    l_data = \"data_%d\"\n",
    "    l_vec = \"vec_%d\"\n",
    "    l_concat = \"concat_%d\"\n",
    "    l_hidden = \"hidden_%d\"\n",
    "    l_relu = \"relu_%d\"\n",
    "    l_output = \"output_%d\"\n",
    "    l_target = \"target_%d\"\n",
    "    l_loss = \"loss_%d\"\n",
    "    \n",
    "    p_vec = [\"p_vec_weight\"]\n",
    "    p_hidden = [\"p_hidden_weight\", \"p_hidden_bias\"]\n",
    "    p_output = [\"p_output_weight\", \"p_output_bias\"]\n",
    "    \n",
    "    net.f(NumpyData(\"seed\", np.zeros((hiddens.shape[0], N_LAYER))))\n",
    "    l_prev = \"seed\"\n",
    "    \n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    for i in range(seq_len):\n",
    "        net.f(NumpyData(l_data % i, observeds[:, i]))\n",
    "        net.f(NumpyData(l_target % i, hiddens[:, i]))\n",
    "        net.f(Wordvec(l_vec % i, N_LAYER, hmm.n_observed,\n",
    "                      bottoms=[l_data % i], param_names=p_vec))\n",
    "        net.f(Concat(l_concat % i, bottoms=[l_vec % i, l_prev]))\n",
    "        net.f(InnerProduct(l_hidden % i, N_LAYER, bottoms=[l_concat % i],\n",
    "                           param_names=p_hidden))\n",
    "        net.f(ReLU(l_relu % i, bottoms=[l_hidden % i]))\n",
    "        l_prev = l_relu % i\n",
    "        net.f(InnerProduct(l_output % i, hmm.n_hidden, \n",
    "                           bottoms=[l_relu % i], param_names=p_output))\n",
    "        loss += net.f(SoftmaxWithLoss(l_loss % i, \n",
    "                                      bottoms=[l_output % i, l_target % i]))\n",
    "        preds = net.blobs[l_output % i].data.argmax(axis=1)\n",
    "        acc += np.mean(preds == hiddens[:, i])\n",
    "        \n",
    "    net.backward()\n",
    "    net.update(lr=0.01)\n",
    "    \n",
    "    return loss / seq_len, acc / seq_len\n",
    "        \n",
    "def iter_bdrnn(hiddens, observeds, hmm, viterbi):\n",
    "    net.clear_forward()\n",
    "    \n",
    "    seq_len = hiddens.shape[1]\n",
    "    \n",
    "    l_data = \"data_%d_bd\"\n",
    "    l_vec = \"vec_%d_bd\"\n",
    "    #\n",
    "    l_concat_f = \"concat_f_%d_bd\"\n",
    "    l_hidden_f = \"hidden_f_%d_bd\"\n",
    "    l_relu_f = \"relu_f_%d_bd\"\n",
    "    #\n",
    "    l_concat_b = \"concat_b_%d_bd\"\n",
    "    l_hidden_b = \"hidden_b_%d_bd\"\n",
    "    l_relu_b = \"relu_b_%d_bd\"\n",
    "    #\n",
    "    l_concat_fb = \"concat_fb_%d_bd\"\n",
    "    l_index = \"index_%d_bd\"\n",
    "    l_output = \"output_%d_bd\"\n",
    "    l_target = \"target_%d_bd\"\n",
    "    l_softmax = \"softmax_%d_bd\"\n",
    "    l_total_logprob = \"total_logprob_bd\"\n",
    "    l_viterbi_target = \"viterbi_target_bd\"\n",
    "    l_viterbi_loss = \"total_loss_bd\"\n",
    "    l_loss = \"loss_%d_bd\"\n",
    "    \n",
    "    p_vec = [\"p_vec_weight_bd\"]\n",
    "    p_hidden_f = [\"p_hidden_f_weight_bd\", \"p_hidden_f_bias_bd\"]\n",
    "    p_hidden_b = [\"p_hidden_b_weight_bd\", \"p_hidden_b_bias_bd\"]\n",
    "    p_output = [\"p_output_weight_bd\", \"p_output_bias_bd\"]\n",
    "    \n",
    "    net.f(NumpyData(\"seed_bd\", np.zeros((hiddens.shape[0], N_LAYER))))\n",
    "    \n",
    "    l_prev = \"seed_bd\"\n",
    "    for i in range(seq_len):\n",
    "        net.f(NumpyData(l_data % i, observeds[:, i]))\n",
    "        net.f(NumpyData(l_target % i, hiddens[:, i]))\n",
    "        net.f(Wordvec(l_vec % i, N_LAYER, hmm.n_observed,\n",
    "                      bottoms=[l_data % i], param_names=p_vec))\n",
    "        \n",
    "        net.f(Concat(l_concat_f % i, bottoms=[l_vec % i, l_prev]))\n",
    "        net.f(InnerProduct(l_hidden_f % i, N_LAYER, bottoms=[l_concat_f % i],\n",
    "                           param_names=p_hidden_f))\n",
    "        net.f(ReLU(l_relu_f % i, bottoms=[l_hidden_f % i]))\n",
    "        l_prev = l_relu_f % i\n",
    "        \n",
    "    l_prev = \"seed_bd\"\n",
    "    for i in reversed(range(seq_len)):\n",
    "        net.f(Concat(l_concat_b % i, bottoms=[l_vec % i, l_prev]))\n",
    "        net.f(InnerProduct(l_hidden_b % i, N_LAYER, bottoms=[l_concat_b % i],\n",
    "                           param_names=p_hidden_b))\n",
    "        net.f(ReLU(l_relu_b % i, bottoms=[l_hidden_b % i]))\n",
    "        l_prev = l_relu_b % i\n",
    "        \n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    if viterbi:\n",
    "        preds = []\n",
    "        for i in range(seq_len):\n",
    "            net.f(Concat(l_concat_fb % i, bottoms=[l_relu_f % i, l_relu_b % i]))\n",
    "            net.f(InnerProduct(l_output % i, hmm.n_hidden, \n",
    "                               bottoms=[l_concat_fb % i], param_names=p_output))\n",
    "            net.f(Softmax(l_softmax % i, bottoms=[l_output % i]))\n",
    "            net.f(Index(l_index % i, {}, bottoms=[l_softmax % i, l_target % i]))\n",
    "            preds.append(net.blobs[l_output % i].data.argmax(axis=1))\n",
    "        # TODO all in logspace\n",
    "        net.f(Eltwise(l_total_logprob, \"PROD\", bottoms=[l_index % i for i in range(seq_len)]))\n",
    "        net.f(NumpyData(l_viterbi_target, np.ones(hiddens.shape[0])))\n",
    "        loss += net.f(SigmoidCrossEntropyLoss(l_viterbi_loss, bottoms=[l_total_logprob, l_viterbi_target]))\n",
    "        preds = np.asarray(preds).T\n",
    "        acc += np.mean((preds == hiddens).min(axis=1))\n",
    "            \n",
    "    else:\n",
    "        for i in range(seq_len):\n",
    "            net.f(Concat(l_concat_fb % i, bottoms=[l_relu_f % i, l_relu_b % i]))\n",
    "            net.f(InnerProduct(l_output % i, hmm.n_hidden, \n",
    "                               bottoms=[l_concat_fb % i], param_names=p_output))\n",
    "            loss += net.f(SoftmaxWithLoss(l_loss % i, \n",
    "                                          bottoms=[l_output % i, l_target % i]))\n",
    "            preds = net.blobs[l_output % i].data.argmax(axis=1)\n",
    "            acc += np.mean(preds == hiddens[:, i])\n",
    "        \n",
    "    net.backward()\n",
    "    net.update(lr=0.01)\n",
    "    \n",
    "    if viterbi:\n",
    "        return loss, acc\n",
    "    else:\n",
    "        return loss / seq_len, acc / seq_len\n",
    "\n",
    "def train_seq_rnn(hmm, backward, viterbi):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    \n",
    "    c_acc = 0\n",
    "    for i_iter in range(2000):\n",
    "        pairs = [sample(5, hmm) for i in range(100)]\n",
    "        hiddens, observeds = zip(*pairs)\n",
    "        hiddens = np.asarray(hiddens)\n",
    "        observeds = np.asarray(observeds)\n",
    "        if backward:\n",
    "            loss_i, acc_i = iter_bdrnn(hiddens, observeds, hmm, viterbi)\n",
    "        else:\n",
    "            assert viterbi == False\n",
    "            loss_i, acc_i = iter_rnn(hiddens, observeds, hmm)\n",
    "        loss += loss_i\n",
    "        acc += acc_i\n",
    "        \n",
    "        c_acc += compute_chart_acc(hiddens, observeds, hmm, backward, viterbi)\n",
    "        \n",
    "        if i_iter % 100 == 0:\n",
    "            print \"%6.2f %6.2f | %6.2f\" % (loss, acc, c_acc)\n",
    "            loss, acc, c_acc = 0, 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.07   0.54 |   0.61\n",
      " 92.33  60.79 |  62.52\n",
      " 86.96  62.11 |  62.56\n",
      " 85.55  62.35 |  62.49\n",
      " 85.21  62.79 |  62.79\n",
      " 84.97  62.62 |  62.64\n",
      " 84.93  62.69 |  62.70\n",
      " 85.09  62.42 |  62.44\n",
      " 85.22  62.57 |  62.60\n",
      " 84.77  62.73 |  62.74\n",
      " 85.13  62.59 |  62.57\n",
      " 85.08  62.17 |  62.16\n",
      " 85.12  62.27 |  62.31\n",
      " 85.33  62.23 |  62.29\n",
      " 84.81  62.69 |  62.80\n",
      " 85.31  62.21 |  62.26\n",
      " 85.67  62.09 |  62.14\n",
      " 85.31  62.28 |  62.31\n",
      " 84.36  62.75 |  62.77\n",
      " 84.52  62.83 |  62.84\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "apollocaffe.set_random_seed(1)\n",
    "hmm = make_hmm(3, 3)\n",
    "train_seq_rnn(hmm, backward=False, viterbi=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.08   0.53 |   0.64\n",
      " 93.33  60.23 |  63.24\n",
      " 87.59  62.11 |  63.22\n",
      " 85.50  62.10 |  63.01\n",
      " 84.74  62.83 |  63.42\n",
      " 84.30  62.98 |  63.14\n",
      " 84.07  63.05 |  63.27\n",
      " 84.10  62.96 |  63.10\n",
      " 84.19  63.07 |  63.19\n",
      " 83.68  63.14 |  63.30\n",
      " 84.12  63.01 |  62.98\n",
      " 84.10  62.73 |  62.69\n",
      " 84.01  62.82 |  62.95\n",
      " 84.28  62.68 |  62.86\n",
      " 83.76  63.02 |  63.08\n",
      " 84.28  62.59 |  62.65\n",
      " 84.69  62.67 |  62.63\n",
      " 84.31  62.68 |  62.63\n",
      " 83.21  63.41 |  63.48\n",
      " 83.41  63.29 |  63.33\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "apollocaffe.set_random_seed(1)\n",
    "hmm = make_hmm(3, 3)\n",
    "train_seq_rnn(hmm, backward=True, viterbi=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
