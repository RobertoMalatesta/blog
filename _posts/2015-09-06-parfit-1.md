---
title: "R&P notes: Theories that are Indirectly Self-Defeating"
categories: [reading]
---

## The self-interest theory

First we distinguish between __formal__ and __substantive__ aims. Formal aims
are "act morally" and "act rationally", and are essentially meta-ethical
principles; substantive aims are the concrete realizations of the formal aims
according to particular theories of morality or rationality.

> It strikes me as somewhat suspect to put "act rationally" in parallel with
> "act morally". It ought to be sufficient to say "act as morally as possible";
> _as possible_ is the entire criterion for rational action. If I have no moral
> aims at all, there is no sense in which I can act irrationally; rational
> behavior is defined with respect to some objective.

One possible substantive aim is that people should act in their self-interest.
This might mean various things (the details don't matter). In summary we say
simply that each person's "supremely rational ultimate aim" is that their life
go as well as possible (for some definition of well).

> I don't understand why the modifier "rational" is necessary to describe this
> ultimate aim. It seems like it should be sufficient to say that preferences
> over world states are well-ordered, and there is some dominating set of world
> states which a person hopes to bring about.

## How S can be indirectly self-defeating

A theory T is __indirectly self-defeating__ if, when someone tries to achieve
their T-given aims, those aims are worse achieved (compared to a world in which
they made no effort at all). Self-defeat is defined with respect to an
individual (more properly an individual and an environment), and is not a
universal property of moral theories. Indirect self-defeat might happen because
an actor is incompetent (and unable to effectively achieve their own
aims)---this case is uninteresting (here DP asserts that the self-interest
theory is "not too difficult to follow"). The more interesting case is where the
actor comes to a worse outcome by effectively pursuing a moral theory. A couple
of examples are given, but the prototype here is the prisoner's dilemma. In
particular, the self-interest theory is indirectly self-defeating for an agent
who always defects, and who advertises to all partners that he will defect.

> The claim that at choosing the self-interest-maximizing action is _easy_ seems
> outrageous---usually, such choices are at best PSPACE-complete. Am I missing
> something? We can design optimization problems that are arbitrarily hard;
> recognizing a good course of action is easier than constructing one (though
> not always itself easy).
>
> W/r/t the prisoner's dilemma, it is possible (though quite strange) to imagine
> an agent that is constitutionally a dominant-strategy player---we have to
> assert that all kinds of pre-commitment mechanisms (like hiring someone to
> murder them if they ever defect) are totally off-limits. So to the extent that
> we are ultimately concerned with _human_ morals, this example seems unhelpful.
> I don't think there are any healthy humans that are totally incapable of
> cooperating under any circumstances.
