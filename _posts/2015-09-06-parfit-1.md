---
title: "R&P notes: Theories that are Indirectly Self-Defeating"
categories: [reading]
---

## The self-interest theory

First we distinguish between __formal__ and __substantive__ aims. Formal aims
are "act morally" and "act rationally", and are essentially meta-ethical
principles; substantive aims are the concrete realizations of the formal aims
according to particular theories of morality or rationality.

> It strikes me as somewhat suspect to put "act rationally" in parallel with
> "act morally". It ought to be sufficient to say "act as morally as possible";
> _as possible_ is the entire criterion for rational action. If I have no moral
> aims at all, there is no sense in which I can act irrationally; rational
> behavior is defined with respect to some objective.

One possible substantive aim is that people should act in their self-interest.
This might mean various things (the details don't matter). In summary we say
simply that each person's "supremely rational ultimate aim" is that their life
go as well as possible (for some definition of well).

> I don't understand why the modifier "rational" is necessary to describe this
> ultimate aim. It seems like it should be sufficient to say that preferences
> over world states are well-ordered, and there is some dominating set of world
> states which a person hopes to bring about.

## How S can be indirectly self-defeating

A theory T is __indirectly self-defeating__ if, when someone tries to achieve
their T-given aims, those aims are worse achieved (compared to a world in which
they made no effort at all). Self-defeat is defined with respect to an
individual (more properly an individual and an environment), and is not a
universal property of moral theories. Indirect self-defeat might happen because
an actor is incompetent (and unable to effectively achieve their own
aims)---this case is uninteresting (here DP asserts that the self-interest
theory is "not too difficult to follow"). 

> The claim that at choosing the self-interest-maximizing action is _easy_ seems
> outrageous---often, such choices are PSPACE-complete! Am I missing
> something? We can design optimization problems that are arbitrarily hard;
> recognizing a good course of action is easier than constructing one (though
> not always itself easy).

The more interesting case is where the
actor comes to a worse outcome by effectively pursuing a moral theory. A couple
of examples are given, but the prototype here is the prisoner's dilemma. In
particular, the self-interest theory is indirectly self-defeating for an agent
who always defects, and who advertises to all partners that he will defect.

> W/r/t the prisoner's dilemma, it is possible (though quite strange) to imagine
> an agent that is constitutionally a dominant-strategy player---we have to
> assert that all kinds of pre-commitment mechanisms (like hiring someone to
> murder them if they ever defect) are totally off-limits. So to the extent that
> we are ultimately concerned with _human_ morals, this example seems unhelpful.
> I don't think there are any healthy humans that are totally incapable of
> cooperating under any circumstances.

## Does S tell us to be never self-denying?

Under the self-interest theory, rationality is precisely the condition of always
acting in one's self-interest. A rational agent ought to maintain only those
beliefs and goals that further their self-interest. These beliefs might be
irrational!  If for some reason I am happier believing in Russell's teapot than
not, I should do whatever is necessary to believe in the teapot. Or, with an
agent for whom (rationally) always following self-interest leads to worse
outcomes, such an agent ought not to behave rationally.

> It seems that we've misread DP w/r/t the prisoner's dilemma
> discussion. Previously we imagined an obligate dominant strategy player, and
> observed that it achieved a worse outcome than it might if it were not
> obligated to play the dominant strategy. Now this agent is apparently 
> choosing between ("rationally") playing the dominant strategy and
> ("irrationally") cooperating. Obviously if cooperating leads to better
> outcomes, it is rational. A few explanations of what might be going on here:
> 1. DP is wrong about game theory
> 2. DP imagines a (more-and-more strangely-constructed) agent who wrongly
>    understands what it means to be rational, and thinks that choosing to be
>    rational requires defection. (We previously dismissed such agents as
>    uninteresting.)
>
> More generally, we seem to be making a distinction between "meta-level"
> rationality (being rational w/r/t/ choice of decisionmaking procedures) and
> "ordinary" rationality (being rational w/r/t non-meta decisions). This
> distinction seems arbitrary and unhelpful, but also necessary to explain why
> it might be "rational to make myself irrational".

Recall that rationality is a formal, not substantive aim; it is a means to
achieve the goals of self-interest, but not (necessarily) itself part of those
goals.

> If this is the only point of the whole preceding discussion, then maybe the
> problems we've raised don't much matter.
